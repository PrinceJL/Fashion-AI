{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0db79e86-8fa5-41e4-93a6-036c0e6937aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5491b7b3-cc2f-4e60-91e8-ebe2eb5f1649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Images Directory exists: ../DeepFashionData\\images\n",
      "âœ“ Captions File exists: ../DeepFashionData\\captions.json\n",
      "âœ“ Shape Labels exists: ../DeepFashionData\\labels\\shape\\shape_anno_all.txt\n",
      "âœ“ Fabric Labels exists: ../DeepFashionData\\labels\\texture\\fabric_ann.txt\n",
      "âœ“ Color Labels exists: ../DeepFashionData\\labels\\texture\\pattern_ann.txt\n",
      "âœ“ Segmentation Directory exists: ../DeepFashionData\\segm\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "base_path = \"../DeepFashionData\"\n",
    "img_dir = os.path.join(base_path, \"images\")\n",
    "text_desc_path = os.path.join(base_path, \"captions.json\")\n",
    "shape_label_path = os.path.join(base_path, \"labels\", \"shape\", \"shape_anno_all.txt\")\n",
    "fabric_label_path = os.path.join(base_path, \"labels\", \"texture\", \"fabric_ann.txt\")\n",
    "color_label_path = os.path.join(base_path, \"labels\", \"texture\", \"pattern_ann.txt\")\n",
    "segm_dir = os.path.join(base_path, \"segm\")\n",
    "\n",
    "# Function to check if all paths exist\n",
    "def check_paths():\n",
    "    paths = {\n",
    "        \"Images Directory\": img_dir,\n",
    "        \"Captions File\": text_desc_path,\n",
    "        \"Shape Labels\": shape_label_path,\n",
    "        \"Fabric Labels\": fabric_label_path,\n",
    "        \"Color Labels\": color_label_path,\n",
    "        \"Segmentation Directory\": segm_dir\n",
    "    }\n",
    "    \n",
    "    for name, path in paths.items():\n",
    "        if os.path.exists(path):\n",
    "            print(f\"âœ“ {name} exists: {path}\")\n",
    "        else:\n",
    "            print(f\"âœ— {name} does not exist: {path}\")\n",
    "\n",
    "# Check all paths before proceeding\n",
    "check_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4cad34b-ea27-4246-a37b-1dfac9de2cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 42544 samples.\n",
      "Sample: MEN-Denim-id_00000080-01_7_additional.jpg [5, 3, 0, 0, 0, 0, 0, 0, 3, 2, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Load annotations\n",
    "def load_shape_labels(label_path):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "\n",
    "    with open(label_path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            parts = line.strip().split()\n",
    "            img_file = parts[0]\n",
    "            shape_attrs = list(map(int, parts[1:]))\n",
    "            image_paths.append(img_file)\n",
    "            labels.append(shape_attrs)\n",
    "    \n",
    "    return image_paths, labels\n",
    "\n",
    "image_paths, labels = load_shape_labels(shape_label_path)\n",
    "print(f\"Loaded {len(image_paths)} samples.\")\n",
    "print(\"Sample:\", image_paths[0], labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1320562b-874e-40e1-b8ce-bd89dc3dde0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class ClothingShapeDataset(Dataset):\n",
    "    def __init__(self, image_dir, image_paths, labels, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_paths[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dda5fad-3fbe-43b3-bc74-935060c2b3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Split the data into training and validation sets (80% train, 20% validation)\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(image_paths, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create dataset objects for train and validation\n",
    "train_dataset = ClothingShapeDataset(img_dir, train_paths, train_labels, transform=transform)\n",
    "val_dataset = ClothingShapeDataset(img_dir, val_paths, val_labels, transform=transform)\n",
    "\n",
    "# Create DataLoaders for train and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Model definition\n",
    "num_classes_per_attr = [6, 5, 4, 3, 5, 3, 3, 3, 5, 7, 3, 3]  # one per attribute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e817933b-4da7-4a5b-b471-a82b7e7a0361",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClothingShapeCNN(nn.Module):\n",
    "    def __init__(self, num_classes_per_attr):\n",
    "        super().__init__()\n",
    "        self.base = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.base.fc = nn.Identity()  # remove final FC layer\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Linear(512, num_classes) for num_classes in num_classes_per_attr\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.base(x)\n",
    "        outputs = [head(features) for head in self.heads]\n",
    "        return outputs  # list of 12 tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdf79e81-0585-43cb-930c-a9be20f980dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = ClothingShapeCNN(num_classes_per_attr)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfe490e0-2d24-458a-b73a-d93a9a9766f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Epoch 1/5 ====================\n",
      "\n",
      "ðŸ”§ Training phase:\n",
      "  â†’ Output (first attribute): tensor([-1.2399,  6.0817,  0.4772, -3.8023, -5.9316, -4.8878],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "  â†’ Label (first sample): tensor([1., 0., 3., 0., 0., 1., 1., 1., 3., 6., 2., 2.])\n",
      "  â†’ Loss: 5.172873497009277\n",
      "  Batch [10/2128], Loss: 4.5665\n",
      "  Batch [20/2128], Loss: 4.4338\n",
      "  Batch [30/2128], Loss: 4.7103\n",
      "  Batch [40/2128], Loss: 2.9666\n",
      "  Batch [50/2128], Loss: 3.5610\n",
      "  Batch [60/2128], Loss: 4.1614\n",
      "  Batch [70/2128], Loss: 4.1504\n",
      "  Batch [80/2128], Loss: 4.7660\n",
      "  Batch [90/2128], Loss: 2.7869\n",
      "  Batch [100/2128], Loss: 7.3895\n",
      "  Batch [110/2128], Loss: 3.8192\n",
      "  Batch [120/2128], Loss: 4.4913\n",
      "  Batch [130/2128], Loss: 4.5400\n",
      "  Batch [140/2128], Loss: 4.5334\n",
      "  Batch [150/2128], Loss: 4.6127\n",
      "  Batch [160/2128], Loss: 4.5089\n",
      "  Batch [170/2128], Loss: 4.0753\n",
      "  Batch [180/2128], Loss: 3.6937\n",
      "  Batch [190/2128], Loss: 4.6265\n",
      "  Batch [200/2128], Loss: 3.9332\n",
      "  Batch [210/2128], Loss: 4.1319\n",
      "  Batch [220/2128], Loss: 4.2922\n",
      "  Batch [230/2128], Loss: 3.1066\n",
      "  Batch [240/2128], Loss: 2.7163\n",
      "  Batch [250/2128], Loss: 4.4183\n",
      "  Batch [260/2128], Loss: 2.9991\n",
      "  Batch [270/2128], Loss: 2.7972\n",
      "  Batch [280/2128], Loss: 3.3246\n",
      "  Batch [290/2128], Loss: 4.4803\n",
      "  Batch [300/2128], Loss: 4.0812\n",
      "  Batch [310/2128], Loss: 4.0183\n",
      "  Batch [320/2128], Loss: 3.8208\n",
      "  Batch [330/2128], Loss: 4.4584\n",
      "  Batch [340/2128], Loss: 5.8412\n",
      "  Batch [350/2128], Loss: 4.4746\n",
      "  Batch [360/2128], Loss: 4.7795\n",
      "  Batch [370/2128], Loss: 3.9206\n",
      "  Batch [380/2128], Loss: 4.3274\n",
      "  Batch [390/2128], Loss: 4.1494\n",
      "  Batch [400/2128], Loss: 5.0006\n",
      "  Batch [410/2128], Loss: 4.1101\n",
      "  Batch [420/2128], Loss: 4.4357\n",
      "  Batch [430/2128], Loss: 4.7707\n",
      "  Batch [440/2128], Loss: 2.8755\n",
      "  Batch [450/2128], Loss: 3.1042\n",
      "  Batch [460/2128], Loss: 5.8490\n",
      "  Batch [470/2128], Loss: 4.1343\n",
      "  Batch [480/2128], Loss: 4.4227\n",
      "  Batch [490/2128], Loss: 5.1026\n",
      "  Batch [500/2128], Loss: 4.6788\n",
      "  Batch [510/2128], Loss: 4.3739\n",
      "  Batch [520/2128], Loss: 3.3987\n",
      "  Batch [530/2128], Loss: 3.9539\n",
      "  Batch [540/2128], Loss: 4.0214\n",
      "  Batch [550/2128], Loss: 3.9180\n",
      "  Batch [560/2128], Loss: 3.9139\n",
      "  Batch [570/2128], Loss: 3.1149\n",
      "  Batch [580/2128], Loss: 2.9934\n",
      "  Batch [590/2128], Loss: 4.9557\n",
      "  Batch [600/2128], Loss: 3.6059\n",
      "  Batch [610/2128], Loss: 4.0004\n",
      "  Batch [620/2128], Loss: 3.7859\n",
      "  Batch [630/2128], Loss: 5.5179\n",
      "  Batch [640/2128], Loss: 3.2090\n",
      "  Batch [650/2128], Loss: 3.1036\n",
      "  Batch [660/2128], Loss: 3.7717\n",
      "  Batch [670/2128], Loss: 4.0125\n",
      "  Batch [680/2128], Loss: 4.2978\n",
      "  Batch [690/2128], Loss: 5.1905\n",
      "  Batch [700/2128], Loss: 3.7162\n",
      "  Batch [710/2128], Loss: 3.8576\n",
      "  Batch [720/2128], Loss: 5.0431\n",
      "  Batch [730/2128], Loss: 3.4442\n",
      "  Batch [740/2128], Loss: 3.5911\n",
      "  Batch [750/2128], Loss: 3.7746\n",
      "  Batch [760/2128], Loss: 4.0358\n",
      "  Batch [770/2128], Loss: 3.3304\n",
      "  Batch [780/2128], Loss: 3.6780\n",
      "  Batch [790/2128], Loss: 3.4930\n",
      "  Batch [800/2128], Loss: 5.2288\n",
      "  Batch [810/2128], Loss: 3.3791\n",
      "  Batch [820/2128], Loss: 2.8017\n",
      "  Batch [830/2128], Loss: 2.9924\n",
      "  Batch [840/2128], Loss: 3.3213\n",
      "  Batch [850/2128], Loss: 3.2887\n",
      "  Batch [860/2128], Loss: 3.9303\n",
      "  Batch [870/2128], Loss: 3.3350\n",
      "  Batch [880/2128], Loss: 2.3302\n",
      "  Batch [890/2128], Loss: 4.2480\n",
      "  Batch [900/2128], Loss: 2.8059\n",
      "  Batch [910/2128], Loss: 2.7089\n",
      "  Batch [920/2128], Loss: 3.2843\n",
      "  Batch [930/2128], Loss: 4.2778\n",
      "  Batch [940/2128], Loss: 3.3529\n",
      "  Batch [950/2128], Loss: 3.9233\n",
      "  Batch [960/2128], Loss: 3.3850\n",
      "  Batch [970/2128], Loss: 4.2177\n",
      "  Batch [980/2128], Loss: 3.3193\n",
      "  Batch [990/2128], Loss: 4.7279\n",
      "  Batch [1000/2128], Loss: 3.9075\n",
      "  Batch [1010/2128], Loss: 4.2254\n",
      "  Batch [1020/2128], Loss: 3.5310\n",
      "  Batch [1030/2128], Loss: 4.4763\n",
      "  Batch [1040/2128], Loss: 4.6329\n",
      "  Batch [1050/2128], Loss: 5.6011\n",
      "  Batch [1060/2128], Loss: 5.8037\n",
      "  Batch [1070/2128], Loss: 3.9819\n",
      "  Batch [1080/2128], Loss: 5.7418\n",
      "  Batch [1090/2128], Loss: 4.8088\n",
      "  Batch [1100/2128], Loss: 2.8442\n",
      "  Batch [1110/2128], Loss: 3.6268\n",
      "  Batch [1120/2128], Loss: 3.7703\n",
      "  Batch [1130/2128], Loss: 3.7490\n",
      "  Batch [1140/2128], Loss: 4.1422\n",
      "  Batch [1150/2128], Loss: 4.6440\n",
      "  Batch [1160/2128], Loss: 2.9863\n",
      "  Batch [1170/2128], Loss: 2.9467\n",
      "  Batch [1180/2128], Loss: 5.1101\n",
      "  Batch [1190/2128], Loss: 2.9674\n",
      "  Batch [1200/2128], Loss: 3.8079\n",
      "  Batch [1210/2128], Loss: 4.1870\n",
      "  Batch [1220/2128], Loss: 2.3332\n",
      "  Batch [1230/2128], Loss: 3.7139\n",
      "  Batch [1240/2128], Loss: 5.1506\n",
      "  Batch [1250/2128], Loss: 3.3268\n",
      "  Batch [1260/2128], Loss: 5.4623\n",
      "  Batch [1270/2128], Loss: 2.7023\n",
      "  Batch [1280/2128], Loss: 3.6433\n",
      "  Batch [1290/2128], Loss: 5.3185\n",
      "  Batch [1300/2128], Loss: 3.0680\n",
      "  Batch [1310/2128], Loss: 4.2947\n",
      "  Batch [1320/2128], Loss: 2.6351\n",
      "  Batch [1330/2128], Loss: 3.6907\n",
      "  Batch [1340/2128], Loss: 3.5402\n",
      "  Batch [1350/2128], Loss: 3.3290\n",
      "  Batch [1360/2128], Loss: 3.8965\n",
      "  Batch [1370/2128], Loss: 5.0982\n",
      "  Batch [1380/2128], Loss: 3.9038\n",
      "  Batch [1390/2128], Loss: 3.4558\n",
      "  Batch [1400/2128], Loss: 2.2900\n",
      "  Batch [1410/2128], Loss: 3.8424\n",
      "  Batch [1420/2128], Loss: 5.0385\n",
      "  Batch [1430/2128], Loss: 5.5529\n",
      "  Batch [1440/2128], Loss: 4.1727\n",
      "  Batch [1450/2128], Loss: 2.2906\n",
      "  Batch [1460/2128], Loss: 4.4637\n",
      "  Batch [1470/2128], Loss: 2.9885\n",
      "  Batch [1480/2128], Loss: 2.6513\n",
      "  Batch [1490/2128], Loss: 4.7799\n",
      "  Batch [1500/2128], Loss: 3.4811\n",
      "  Batch [1510/2128], Loss: 3.5926\n",
      "  Batch [1520/2128], Loss: 3.0831\n",
      "  Batch [1530/2128], Loss: 3.2242\n",
      "  Batch [1540/2128], Loss: 3.2090\n",
      "  Batch [1550/2128], Loss: 5.7523\n",
      "  Batch [1560/2128], Loss: 4.8144\n",
      "  Batch [1570/2128], Loss: 3.6730\n",
      "  Batch [1580/2128], Loss: 5.0842\n",
      "  Batch [1590/2128], Loss: 3.4647\n",
      "  Batch [1600/2128], Loss: 3.4002\n",
      "  Batch [1610/2128], Loss: 2.5998\n",
      "  Batch [1620/2128], Loss: 4.5079\n",
      "  Batch [1630/2128], Loss: 3.6263\n",
      "  Batch [1640/2128], Loss: 3.9282\n",
      "  Batch [1650/2128], Loss: 3.9341\n",
      "  Batch [1660/2128], Loss: 3.1233\n",
      "  Batch [1670/2128], Loss: 3.6377\n",
      "  Batch [1680/2128], Loss: 3.6404\n",
      "  Batch [1690/2128], Loss: 3.4599\n",
      "  Batch [1700/2128], Loss: 3.4544\n",
      "  Batch [1710/2128], Loss: 3.1502\n",
      "  Batch [1720/2128], Loss: 3.9325\n",
      "  Batch [1730/2128], Loss: 3.9776\n",
      "  Batch [1740/2128], Loss: 5.1944\n",
      "  Batch [1750/2128], Loss: 3.5058\n",
      "  Batch [1760/2128], Loss: 4.7171\n",
      "  Batch [1770/2128], Loss: 4.5306\n",
      "  Batch [1780/2128], Loss: 6.0715\n",
      "  Batch [1790/2128], Loss: 4.9590\n",
      "  Batch [1800/2128], Loss: 4.2523\n",
      "  Batch [1810/2128], Loss: 3.1124\n",
      "  Batch [1820/2128], Loss: 4.3578\n",
      "  Batch [1830/2128], Loss: 3.8500\n",
      "  Batch [1840/2128], Loss: 3.6344\n",
      "  Batch [1850/2128], Loss: 4.1865\n",
      "  Batch [1860/2128], Loss: 3.2839\n",
      "  Batch [1870/2128], Loss: 5.0027\n",
      "  Batch [1880/2128], Loss: 3.9048\n",
      "  Batch [1890/2128], Loss: 2.9632\n",
      "  Batch [1900/2128], Loss: 3.3645\n",
      "  Batch [1910/2128], Loss: 3.8835\n",
      "  Batch [1920/2128], Loss: 2.9835\n",
      "  Batch [1930/2128], Loss: 2.1683\n",
      "  Batch [1940/2128], Loss: 4.2007\n",
      "  Batch [1950/2128], Loss: 4.0406\n",
      "  Batch [1960/2128], Loss: 3.7382\n",
      "  Batch [1970/2128], Loss: 3.4543\n",
      "  Batch [1980/2128], Loss: 4.2946\n",
      "  Batch [1990/2128], Loss: 3.4906\n",
      "  Batch [2000/2128], Loss: 2.2436\n",
      "  Batch [2010/2128], Loss: 5.4658\n",
      "  Batch [2020/2128], Loss: 4.5680\n",
      "  Batch [2030/2128], Loss: 3.2449\n",
      "  Batch [2040/2128], Loss: 3.7784\n",
      "  Batch [2050/2128], Loss: 3.5884\n",
      "  Batch [2060/2128], Loss: 4.2771\n",
      "  Batch [2070/2128], Loss: 4.3406\n",
      "  Batch [2080/2128], Loss: 3.0682\n",
      "  Batch [2090/2128], Loss: 3.6835\n",
      "  Batch [2100/2128], Loss: 3.2146\n",
      "  Batch [2110/2128], Loss: 4.4811\n",
      "  Batch [2120/2128], Loss: 3.2915\n",
      "  Batch [2128/2128], Loss: 7.0089\n",
      "\n",
      "ðŸ“Š Training Accuracy per Attribute:\n",
      "  Attribute 0: 0.9275\n",
      "  Attribute 1: 0.9522\n",
      "  Attribute 2: 0.9449\n",
      "  Attribute 3: 0.9706\n",
      "  Attribute 4: 0.9733\n",
      "  Attribute 5: 0.7263\n",
      "  Attribute 6: 0.8239\n",
      "  Attribute 7: 0.7639\n",
      "  Attribute 8: 0.8363\n",
      "  Attribute 9: 0.7924\n",
      "  Attribute 10: 0.9030\n",
      "  Attribute 11: 0.9154\n",
      "\n",
      "âœ… Epoch 1 Training Complete.\n",
      "  Average Loss: 3.9481\n",
      "  Overall Training Accuracy: 0.8775\n",
      "\n",
      "ðŸ§ª Validation phase:\n",
      "\n",
      "ðŸ“Š Validation Accuracy per Attribute:\n",
      "  Attribute 0 (val): 0.9337\n",
      "  Attribute 1 (val): 0.9593\n",
      "  Attribute 2 (val): 0.9515\n",
      "  Attribute 3 (val): 0.9676\n",
      "  Attribute 4 (val): 0.9537\n",
      "  Attribute 5 (val): 0.7134\n",
      "  Attribute 6 (val): 0.8376\n",
      "  Attribute 7 (val): 0.7706\n",
      "  Attribute 8 (val): 0.8402\n",
      "  Attribute 9 (val): 0.7469\n",
      "  Attribute 10 (val): 0.9066\n",
      "  Attribute 11 (val): 0.9223\n",
      "\n",
      "ðŸ“‰ Validation Loss: 4.0328\n",
      "âœ… Overall Validation Accuracy: 0.8753\n",
      "\n",
      "==================== Epoch 2/5 ====================\n",
      "\n",
      "ðŸ”§ Training phase:\n",
      "  Batch [10/2128], Loss: 3.5601\n",
      "  Batch [20/2128], Loss: 4.6233\n",
      "  Batch [30/2128], Loss: 2.9130\n",
      "  Batch [40/2128], Loss: 2.2689\n",
      "  Batch [50/2128], Loss: 2.7759\n",
      "  Batch [60/2128], Loss: 3.2577\n",
      "  Batch [70/2128], Loss: 3.3853\n",
      "  Batch [80/2128], Loss: 3.4537\n",
      "  Batch [90/2128], Loss: 4.8900\n",
      "  Batch [100/2128], Loss: 2.7837\n",
      "  Batch [110/2128], Loss: 4.4156\n",
      "  Batch [120/2128], Loss: 3.6195\n",
      "  Batch [130/2128], Loss: 2.8294\n",
      "  Batch [140/2128], Loss: 4.2315\n",
      "  Batch [150/2128], Loss: 3.2850\n",
      "  Batch [160/2128], Loss: 3.7517\n",
      "  Batch [170/2128], Loss: 2.6789\n",
      "  Batch [180/2128], Loss: 3.0239\n",
      "  Batch [190/2128], Loss: 2.9979\n",
      "  Batch [200/2128], Loss: 3.1714\n",
      "  Batch [210/2128], Loss: 3.2685\n",
      "  Batch [220/2128], Loss: 2.1809\n",
      "  Batch [230/2128], Loss: 4.5551\n",
      "  Batch [240/2128], Loss: 3.0712\n",
      "  Batch [250/2128], Loss: 2.1983\n",
      "  Batch [260/2128], Loss: 3.4850\n",
      "  Batch [270/2128], Loss: 3.1740\n",
      "  Batch [280/2128], Loss: 2.6799\n",
      "  Batch [290/2128], Loss: 3.3434\n",
      "  Batch [300/2128], Loss: 2.7191\n",
      "  Batch [310/2128], Loss: 3.2671\n",
      "  Batch [320/2128], Loss: 3.0987\n",
      "  Batch [330/2128], Loss: 3.5922\n",
      "  Batch [340/2128], Loss: 2.9264\n",
      "  Batch [350/2128], Loss: 3.7018\n",
      "  Batch [360/2128], Loss: 3.8390\n",
      "  Batch [370/2128], Loss: 4.4801\n",
      "  Batch [380/2128], Loss: 3.8513\n",
      "  Batch [390/2128], Loss: 4.4640\n",
      "  Batch [400/2128], Loss: 3.1437\n",
      "  Batch [410/2128], Loss: 3.6175\n",
      "  Batch [420/2128], Loss: 3.3352\n",
      "  Batch [430/2128], Loss: 3.8309\n",
      "  Batch [440/2128], Loss: 3.2718\n",
      "  Batch [450/2128], Loss: 2.9599\n",
      "  Batch [460/2128], Loss: 2.3924\n",
      "  Batch [470/2128], Loss: 4.6424\n",
      "  Batch [480/2128], Loss: 2.3490\n",
      "  Batch [490/2128], Loss: 2.8632\n",
      "  Batch [500/2128], Loss: 2.7470\n",
      "  Batch [510/2128], Loss: 3.7172\n",
      "  Batch [520/2128], Loss: 3.3437\n",
      "  Batch [530/2128], Loss: 2.9256\n",
      "  Batch [540/2128], Loss: 2.7864\n",
      "  Batch [550/2128], Loss: 3.2149\n",
      "  Batch [560/2128], Loss: 4.2996\n",
      "  Batch [570/2128], Loss: 3.1284\n",
      "  Batch [580/2128], Loss: 2.7841\n",
      "  Batch [590/2128], Loss: 2.3457\n",
      "  Batch [600/2128], Loss: 4.3051\n",
      "  Batch [610/2128], Loss: 3.5984\n",
      "  Batch [620/2128], Loss: 2.8123\n",
      "  Batch [630/2128], Loss: 2.8061\n",
      "  Batch [640/2128], Loss: 3.4885\n",
      "  Batch [650/2128], Loss: 4.8179\n",
      "  Batch [660/2128], Loss: 3.1327\n",
      "  Batch [670/2128], Loss: 2.9774\n",
      "  Batch [680/2128], Loss: 3.5748\n",
      "  Batch [690/2128], Loss: 3.4921\n",
      "  Batch [700/2128], Loss: 3.7086\n",
      "  Batch [710/2128], Loss: 3.3594\n",
      "  Batch [720/2128], Loss: 3.0412\n",
      "  Batch [730/2128], Loss: 4.2904\n",
      "  Batch [740/2128], Loss: 5.1900\n",
      "  Batch [750/2128], Loss: 3.8117\n",
      "  Batch [760/2128], Loss: 2.6156\n",
      "  Batch [770/2128], Loss: 3.5362\n",
      "  Batch [780/2128], Loss: 3.4027\n",
      "  Batch [790/2128], Loss: 4.2475\n",
      "  Batch [800/2128], Loss: 3.1941\n",
      "  Batch [810/2128], Loss: 3.9285\n",
      "  Batch [820/2128], Loss: 3.4380\n",
      "  Batch [830/2128], Loss: 3.4879\n",
      "  Batch [840/2128], Loss: 2.6018\n",
      "  Batch [850/2128], Loss: 3.5926\n",
      "  Batch [860/2128], Loss: 2.2569\n",
      "  Batch [870/2128], Loss: 4.3548\n",
      "  Batch [880/2128], Loss: 2.3416\n",
      "  Batch [890/2128], Loss: 2.5088\n",
      "  Batch [900/2128], Loss: 4.5885\n",
      "  Batch [910/2128], Loss: 3.4663\n",
      "  Batch [920/2128], Loss: 3.9880\n",
      "  Batch [930/2128], Loss: 3.1650\n",
      "  Batch [940/2128], Loss: 4.9634\n",
      "  Batch [950/2128], Loss: 3.4070\n",
      "  Batch [960/2128], Loss: 4.5011\n",
      "  Batch [970/2128], Loss: 3.6621\n",
      "  Batch [980/2128], Loss: 3.5639\n",
      "  Batch [990/2128], Loss: 3.2553\n",
      "  Batch [1000/2128], Loss: 3.5323\n",
      "  Batch [1010/2128], Loss: 2.5112\n",
      "  Batch [1020/2128], Loss: 3.2830\n",
      "  Batch [1030/2128], Loss: 3.2721\n",
      "  Batch [1040/2128], Loss: 4.3165\n",
      "  Batch [1050/2128], Loss: 3.9613\n",
      "  Batch [1060/2128], Loss: 2.9432\n",
      "  Batch [1070/2128], Loss: 2.9195\n",
      "  Batch [1080/2128], Loss: 4.1156\n",
      "  Batch [1090/2128], Loss: 2.9717\n",
      "  Batch [1100/2128], Loss: 3.9147\n",
      "  Batch [1110/2128], Loss: 2.7079\n",
      "  Batch [1120/2128], Loss: 3.6900\n",
      "  Batch [1130/2128], Loss: 3.0876\n",
      "  Batch [1140/2128], Loss: 2.7980\n",
      "  Batch [1150/2128], Loss: 3.8841\n",
      "  Batch [1160/2128], Loss: 3.1742\n",
      "  Batch [1170/2128], Loss: 3.3432\n",
      "  Batch [1180/2128], Loss: 3.9472\n",
      "  Batch [1190/2128], Loss: 3.7996\n",
      "  Batch [1200/2128], Loss: 2.8452\n",
      "  Batch [1210/2128], Loss: 3.7429\n",
      "  Batch [1220/2128], Loss: 2.1688\n",
      "  Batch [1230/2128], Loss: 2.7717\n",
      "  Batch [1240/2128], Loss: 3.9113\n",
      "  Batch [1250/2128], Loss: 3.5471\n",
      "  Batch [1260/2128], Loss: 3.4419\n",
      "  Batch [1270/2128], Loss: 2.9328\n",
      "  Batch [1280/2128], Loss: 2.8806\n",
      "  Batch [1290/2128], Loss: 3.6827\n",
      "  Batch [1300/2128], Loss: 3.9793\n",
      "  Batch [1310/2128], Loss: 3.6496\n",
      "  Batch [1320/2128], Loss: 3.1884\n",
      "  Batch [1330/2128], Loss: 3.4639\n",
      "  Batch [1340/2128], Loss: 4.7804\n",
      "  Batch [1350/2128], Loss: 3.7123\n",
      "  Batch [1360/2128], Loss: 3.2688\n",
      "  Batch [1370/2128], Loss: 1.6703\n",
      "  Batch [1380/2128], Loss: 3.9258\n",
      "  Batch [1390/2128], Loss: 2.5970\n",
      "  Batch [1400/2128], Loss: 2.4202\n",
      "  Batch [1410/2128], Loss: 2.4691\n",
      "  Batch [1420/2128], Loss: 5.8577\n",
      "  Batch [1430/2128], Loss: 3.3996\n",
      "  Batch [1440/2128], Loss: 3.1340\n",
      "  Batch [1450/2128], Loss: 2.2172\n",
      "  Batch [1460/2128], Loss: 2.7142\n",
      "  Batch [1470/2128], Loss: 2.8743\n",
      "  Batch [1480/2128], Loss: 3.1658\n",
      "  Batch [1490/2128], Loss: 3.0725\n",
      "  Batch [1500/2128], Loss: 3.8057\n",
      "  Batch [1510/2128], Loss: 3.3160\n",
      "  Batch [1520/2128], Loss: 3.5780\n",
      "  Batch [1530/2128], Loss: 2.8850\n",
      "  Batch [1540/2128], Loss: 3.0858\n",
      "  Batch [1550/2128], Loss: 2.2898\n",
      "  Batch [1560/2128], Loss: 2.9582\n",
      "  Batch [1570/2128], Loss: 4.5959\n",
      "  Batch [1580/2128], Loss: 3.6302\n",
      "  Batch [1590/2128], Loss: 3.5853\n",
      "  Batch [1600/2128], Loss: 3.1497\n",
      "  Batch [1610/2128], Loss: 4.2024\n",
      "  Batch [1620/2128], Loss: 3.3577\n",
      "  Batch [1630/2128], Loss: 3.6976\n",
      "  Batch [1640/2128], Loss: 2.9853\n",
      "  Batch [1650/2128], Loss: 4.1850\n",
      "  Batch [1660/2128], Loss: 2.8314\n",
      "  Batch [1670/2128], Loss: 3.1103\n",
      "  Batch [1680/2128], Loss: 4.8539\n",
      "  Batch [1690/2128], Loss: 2.9269\n",
      "  Batch [1700/2128], Loss: 4.0219\n",
      "  Batch [1710/2128], Loss: 3.3186\n",
      "  Batch [1720/2128], Loss: 2.9654\n",
      "  Batch [1730/2128], Loss: 3.6341\n",
      "  Batch [1740/2128], Loss: 3.2938\n",
      "  Batch [1750/2128], Loss: 3.8048\n",
      "  Batch [1760/2128], Loss: 2.0211\n",
      "  Batch [1770/2128], Loss: 3.2590\n",
      "  Batch [1780/2128], Loss: 3.3540\n",
      "  Batch [1790/2128], Loss: 3.4252\n",
      "  Batch [1800/2128], Loss: 3.0441\n",
      "  Batch [1810/2128], Loss: 3.1426\n",
      "  Batch [1820/2128], Loss: 4.2219\n",
      "  Batch [1830/2128], Loss: 3.3615\n",
      "  Batch [1840/2128], Loss: 3.9259\n",
      "  Batch [1850/2128], Loss: 3.5979\n",
      "  Batch [1860/2128], Loss: 4.4089\n",
      "  Batch [1870/2128], Loss: 2.8011\n",
      "  Batch [1880/2128], Loss: 3.3306\n",
      "  Batch [1890/2128], Loss: 3.1288\n",
      "  Batch [1900/2128], Loss: 4.4137\n",
      "  Batch [1910/2128], Loss: 3.7910\n",
      "  Batch [1920/2128], Loss: 2.9300\n",
      "  Batch [1930/2128], Loss: 2.9352\n",
      "  Batch [1940/2128], Loss: 2.9610\n",
      "  Batch [1950/2128], Loss: 2.7624\n",
      "  Batch [1960/2128], Loss: 1.8376\n",
      "  Batch [1970/2128], Loss: 3.7136\n",
      "  Batch [1980/2128], Loss: 3.5838\n",
      "  Batch [1990/2128], Loss: 3.9670\n",
      "  Batch [2000/2128], Loss: 4.0308\n",
      "  Batch [2010/2128], Loss: 2.8416\n",
      "  Batch [2020/2128], Loss: 3.8974\n",
      "  Batch [2030/2128], Loss: 2.0543\n",
      "  Batch [2040/2128], Loss: 3.9769\n",
      "  Batch [2050/2128], Loss: 3.0662\n",
      "  Batch [2060/2128], Loss: 3.6529\n",
      "  Batch [2070/2128], Loss: 3.5813\n",
      "  Batch [2080/2128], Loss: 2.9087\n",
      "  Batch [2090/2128], Loss: 2.4982\n",
      "  Batch [2100/2128], Loss: 2.3314\n",
      "  Batch [2110/2128], Loss: 3.8110\n",
      "  Batch [2120/2128], Loss: 4.9895\n",
      "  Batch [2128/2128], Loss: 2.7880\n",
      "\n",
      "ðŸ“Š Training Accuracy per Attribute:\n",
      "  Attribute 0: 0.9386\n",
      "  Attribute 1: 0.9614\n",
      "  Attribute 2: 0.9584\n",
      "  Attribute 3: 0.9758\n",
      "  Attribute 4: 0.9773\n",
      "  Attribute 5: 0.7680\n",
      "  Attribute 6: 0.8539\n",
      "  Attribute 7: 0.7897\n",
      "  Attribute 8: 0.8540\n",
      "  Attribute 9: 0.8246\n",
      "  Attribute 10: 0.9196\n",
      "  Attribute 11: 0.9283\n",
      "\n",
      "âœ… Epoch 2 Training Complete.\n",
      "  Average Loss: 3.3892\n",
      "  Overall Training Accuracy: 0.8958\n",
      "\n",
      "ðŸ§ª Validation phase:\n",
      "\n",
      "ðŸ“Š Validation Accuracy per Attribute:\n",
      "  Attribute 0 (val): 0.9311\n",
      "  Attribute 1 (val): 0.9612\n",
      "  Attribute 2 (val): 0.9542\n",
      "  Attribute 3 (val): 0.9421\n",
      "  Attribute 4 (val): 0.9764\n",
      "  Attribute 5 (val): 0.7375\n",
      "  Attribute 6 (val): 0.8513\n",
      "  Attribute 7 (val): 0.7855\n",
      "  Attribute 8 (val): 0.8553\n",
      "  Attribute 9 (val): 0.8182\n",
      "  Attribute 10 (val): 0.9157\n",
      "  Attribute 11 (val): 0.9249\n",
      "\n",
      "ðŸ“‰ Validation Loss: 3.5836\n",
      "âœ… Overall Validation Accuracy: 0.8878\n",
      "\n",
      "==================== Epoch 3/5 ====================\n",
      "\n",
      "ðŸ”§ Training phase:\n",
      "  Batch [10/2128], Loss: 2.7242\n",
      "  Batch [20/2128], Loss: 2.8709\n",
      "  Batch [30/2128], Loss: 2.4733\n",
      "  Batch [40/2128], Loss: 2.4743\n",
      "  Batch [50/2128], Loss: 4.0589\n",
      "  Batch [60/2128], Loss: 1.7719\n",
      "  Batch [70/2128], Loss: 3.3606\n",
      "  Batch [80/2128], Loss: 2.5554\n",
      "  Batch [90/2128], Loss: 1.9566\n",
      "  Batch [100/2128], Loss: 2.0717\n",
      "  Batch [110/2128], Loss: 2.5750\n",
      "  Batch [120/2128], Loss: 2.6133\n",
      "  Batch [130/2128], Loss: 2.5117\n",
      "  Batch [140/2128], Loss: 3.4554\n",
      "  Batch [150/2128], Loss: 2.7500\n",
      "  Batch [160/2128], Loss: 2.9030\n",
      "  Batch [170/2128], Loss: 2.7399\n",
      "  Batch [180/2128], Loss: 2.5835\n",
      "  Batch [190/2128], Loss: 3.9142\n",
      "  Batch [200/2128], Loss: 2.6808\n",
      "  Batch [210/2128], Loss: 2.5096\n",
      "  Batch [220/2128], Loss: 2.9384\n",
      "  Batch [230/2128], Loss: 2.9738\n",
      "  Batch [240/2128], Loss: 2.7429\n",
      "  Batch [250/2128], Loss: 4.5894\n",
      "  Batch [260/2128], Loss: 2.6782\n",
      "  Batch [270/2128], Loss: 3.5152\n",
      "  Batch [280/2128], Loss: 3.1379\n",
      "  Batch [290/2128], Loss: 2.7623\n",
      "  Batch [300/2128], Loss: 2.6689\n",
      "  Batch [310/2128], Loss: 2.4619\n",
      "  Batch [320/2128], Loss: 3.4970\n",
      "  Batch [330/2128], Loss: 3.8674\n",
      "  Batch [340/2128], Loss: 2.8848\n",
      "  Batch [350/2128], Loss: 3.6245\n",
      "  Batch [360/2128], Loss: 2.8359\n",
      "  Batch [370/2128], Loss: 1.7759\n",
      "  Batch [380/2128], Loss: 2.4820\n",
      "  Batch [390/2128], Loss: 3.0302\n",
      "  Batch [400/2128], Loss: 2.9436\n",
      "  Batch [410/2128], Loss: 2.7268\n",
      "  Batch [420/2128], Loss: 2.3378\n",
      "  Batch [430/2128], Loss: 3.0351\n",
      "  Batch [440/2128], Loss: 3.3430\n",
      "  Batch [450/2128], Loss: 4.0114\n",
      "  Batch [460/2128], Loss: 3.0492\n",
      "  Batch [470/2128], Loss: 3.2676\n",
      "  Batch [480/2128], Loss: 2.9683\n",
      "  Batch [490/2128], Loss: 2.4363\n",
      "  Batch [500/2128], Loss: 2.9401\n",
      "  Batch [510/2128], Loss: 2.7514\n",
      "  Batch [520/2128], Loss: 3.2168\n",
      "  Batch [530/2128], Loss: 2.6407\n",
      "  Batch [540/2128], Loss: 3.2669\n",
      "  Batch [550/2128], Loss: 3.7993\n",
      "  Batch [560/2128], Loss: 2.7580\n",
      "  Batch [570/2128], Loss: 2.5587\n",
      "  Batch [580/2128], Loss: 2.9994\n",
      "  Batch [590/2128], Loss: 3.3007\n",
      "  Batch [600/2128], Loss: 2.8587\n",
      "  Batch [610/2128], Loss: 3.3562\n",
      "  Batch [620/2128], Loss: 3.2182\n",
      "  Batch [630/2128], Loss: 2.8914\n",
      "  Batch [640/2128], Loss: 2.1798\n",
      "  Batch [650/2128], Loss: 3.0332\n",
      "  Batch [660/2128], Loss: 2.6534\n",
      "  Batch [670/2128], Loss: 2.1719\n",
      "  Batch [680/2128], Loss: 2.5227\n",
      "  Batch [690/2128], Loss: 2.6831\n",
      "  Batch [700/2128], Loss: 2.4824\n",
      "  Batch [710/2128], Loss: 1.9693\n",
      "  Batch [720/2128], Loss: 2.8070\n",
      "  Batch [730/2128], Loss: 2.3437\n",
      "  Batch [740/2128], Loss: 2.2043\n",
      "  Batch [750/2128], Loss: 3.2930\n",
      "  Batch [760/2128], Loss: 2.9828\n",
      "  Batch [770/2128], Loss: 3.0595\n",
      "  Batch [780/2128], Loss: 2.6719\n",
      "  Batch [790/2128], Loss: 3.0997\n",
      "  Batch [800/2128], Loss: 2.6414\n",
      "  Batch [810/2128], Loss: 3.0634\n",
      "  Batch [820/2128], Loss: 1.8476\n",
      "  Batch [830/2128], Loss: 3.4950\n",
      "  Batch [840/2128], Loss: 1.9934\n",
      "  Batch [850/2128], Loss: 3.5166\n",
      "  Batch [860/2128], Loss: 4.1510\n",
      "  Batch [870/2128], Loss: 2.4796\n",
      "  Batch [880/2128], Loss: 2.5478\n",
      "  Batch [890/2128], Loss: 2.9026\n",
      "  Batch [900/2128], Loss: 3.4275\n",
      "  Batch [910/2128], Loss: 2.7641\n",
      "  Batch [920/2128], Loss: 3.2030\n",
      "  Batch [930/2128], Loss: 3.0575\n",
      "  Batch [940/2128], Loss: 2.6781\n",
      "  Batch [950/2128], Loss: 3.8189\n",
      "  Batch [960/2128], Loss: 2.5002\n",
      "  Batch [970/2128], Loss: 2.9313\n",
      "  Batch [980/2128], Loss: 3.1612\n",
      "  Batch [990/2128], Loss: 2.2519\n",
      "  Batch [1000/2128], Loss: 3.0432\n",
      "  Batch [1010/2128], Loss: 2.9608\n",
      "  Batch [1020/2128], Loss: 3.0829\n",
      "  Batch [1030/2128], Loss: 3.8904\n",
      "  Batch [1040/2128], Loss: 3.4088\n",
      "  Batch [1050/2128], Loss: 3.2688\n",
      "  Batch [1060/2128], Loss: 2.4277\n",
      "  Batch [1070/2128], Loss: 3.4927\n",
      "  Batch [1080/2128], Loss: 2.8492\n",
      "  Batch [1090/2128], Loss: 4.0914\n",
      "  Batch [1100/2128], Loss: 4.0679\n",
      "  Batch [1110/2128], Loss: 3.0534\n",
      "  Batch [1120/2128], Loss: 2.7194\n",
      "  Batch [1130/2128], Loss: 3.4188\n",
      "  Batch [1140/2128], Loss: 3.1082\n",
      "  Batch [1150/2128], Loss: 2.6158\n",
      "  Batch [1160/2128], Loss: 3.2900\n",
      "  Batch [1170/2128], Loss: 3.1817\n",
      "  Batch [1180/2128], Loss: 2.9671\n",
      "  Batch [1190/2128], Loss: 2.7488\n",
      "  Batch [1200/2128], Loss: 2.0029\n",
      "  Batch [1210/2128], Loss: 4.6158\n",
      "  Batch [1220/2128], Loss: 3.7767\n",
      "  Batch [1230/2128], Loss: 3.0160\n",
      "  Batch [1240/2128], Loss: 2.1838\n",
      "  Batch [1250/2128], Loss: 3.7905\n",
      "  Batch [1260/2128], Loss: 3.2366\n",
      "  Batch [1270/2128], Loss: 2.6314\n",
      "  Batch [1280/2128], Loss: 3.8144\n",
      "  Batch [1290/2128], Loss: 2.8792\n",
      "  Batch [1300/2128], Loss: 2.2225\n",
      "  Batch [1310/2128], Loss: 3.4043\n",
      "  Batch [1320/2128], Loss: 3.1157\n",
      "  Batch [1330/2128], Loss: 3.2124\n",
      "  Batch [1340/2128], Loss: 2.9405\n",
      "  Batch [1350/2128], Loss: 3.5889\n",
      "  Batch [1360/2128], Loss: 3.2751\n",
      "  Batch [1370/2128], Loss: 2.8404\n",
      "  Batch [1380/2128], Loss: 2.5111\n",
      "  Batch [1390/2128], Loss: 2.6705\n",
      "  Batch [1400/2128], Loss: 3.2655\n",
      "  Batch [1410/2128], Loss: 3.1228\n",
      "  Batch [1420/2128], Loss: 4.2049\n",
      "  Batch [1430/2128], Loss: 3.1588\n",
      "  Batch [1440/2128], Loss: 3.0534\n",
      "  Batch [1450/2128], Loss: 2.3910\n",
      "  Batch [1460/2128], Loss: 3.0367\n",
      "  Batch [1470/2128], Loss: 3.3752\n",
      "  Batch [1480/2128], Loss: 2.9118\n",
      "  Batch [1490/2128], Loss: 3.5344\n",
      "  Batch [1500/2128], Loss: 2.8837\n",
      "  Batch [1510/2128], Loss: 2.1065\n",
      "  Batch [1520/2128], Loss: 3.3087\n",
      "  Batch [1530/2128], Loss: 2.8988\n",
      "  Batch [1540/2128], Loss: 2.9108\n",
      "  Batch [1550/2128], Loss: 2.8020\n",
      "  Batch [1560/2128], Loss: 2.3227\n",
      "  Batch [1570/2128], Loss: 3.4052\n",
      "  Batch [1580/2128], Loss: 3.5277\n",
      "  Batch [1590/2128], Loss: 2.7085\n",
      "  Batch [1600/2128], Loss: 2.1381\n",
      "  Batch [1610/2128], Loss: 4.2151\n",
      "  Batch [1620/2128], Loss: 3.2844\n",
      "  Batch [1630/2128], Loss: 3.4100\n",
      "  Batch [1640/2128], Loss: 2.8520\n",
      "  Batch [1650/2128], Loss: 2.1845\n",
      "  Batch [1660/2128], Loss: 5.0658\n",
      "  Batch [1670/2128], Loss: 2.0635\n",
      "  Batch [1680/2128], Loss: 2.4304\n",
      "  Batch [1690/2128], Loss: 2.7902\n",
      "  Batch [1700/2128], Loss: 2.5247\n",
      "  Batch [1710/2128], Loss: 3.4686\n",
      "  Batch [1720/2128], Loss: 2.2971\n",
      "  Batch [1730/2128], Loss: 1.7484\n",
      "  Batch [1740/2128], Loss: 2.9241\n",
      "  Batch [1750/2128], Loss: 2.5594\n",
      "  Batch [1760/2128], Loss: 3.6345\n",
      "  Batch [1770/2128], Loss: 3.5084\n",
      "  Batch [1780/2128], Loss: 3.1253\n",
      "  Batch [1790/2128], Loss: 2.4790\n",
      "  Batch [1800/2128], Loss: 2.4263\n",
      "  Batch [1810/2128], Loss: 2.5329\n",
      "  Batch [1820/2128], Loss: 2.4071\n",
      "  Batch [1830/2128], Loss: 3.0550\n",
      "  Batch [1840/2128], Loss: 3.3435\n",
      "  Batch [1850/2128], Loss: 2.1895\n",
      "  Batch [1860/2128], Loss: 3.3224\n",
      "  Batch [1870/2128], Loss: 2.5104\n",
      "  Batch [1880/2128], Loss: 2.4016\n",
      "  Batch [1890/2128], Loss: 3.6610\n",
      "  Batch [1900/2128], Loss: 3.1580\n",
      "  Batch [1910/2128], Loss: 2.5452\n",
      "  Batch [1920/2128], Loss: 3.1991\n",
      "  Batch [1930/2128], Loss: 2.9829\n",
      "  Batch [1940/2128], Loss: 2.6416\n",
      "  Batch [1950/2128], Loss: 3.3033\n",
      "  Batch [1960/2128], Loss: 3.1802\n",
      "  Batch [1970/2128], Loss: 2.4159\n",
      "  Batch [1980/2128], Loss: 3.7895\n",
      "  Batch [1990/2128], Loss: 3.1690\n",
      "  Batch [2000/2128], Loss: 3.2052\n",
      "  Batch [2010/2128], Loss: 2.0177\n",
      "  Batch [2020/2128], Loss: 1.6398\n",
      "  Batch [2030/2128], Loss: 2.3271\n",
      "  Batch [2040/2128], Loss: 4.1224\n",
      "  Batch [2050/2128], Loss: 3.9209\n",
      "  Batch [2060/2128], Loss: 1.8298\n",
      "  Batch [2070/2128], Loss: 3.6663\n",
      "  Batch [2080/2128], Loss: 2.3194\n",
      "  Batch [2090/2128], Loss: 2.5887\n",
      "  Batch [2100/2128], Loss: 1.6579\n",
      "  Batch [2110/2128], Loss: 4.1148\n",
      "  Batch [2120/2128], Loss: 2.9894\n",
      "  Batch [2128/2128], Loss: 3.0399\n",
      "\n",
      "ðŸ“Š Training Accuracy per Attribute:\n",
      "  Attribute 0: 0.9480\n",
      "  Attribute 1: 0.9663\n",
      "  Attribute 2: 0.9656\n",
      "  Attribute 3: 0.9800\n",
      "  Attribute 4: 0.9805\n",
      "  Attribute 5: 0.7953\n",
      "  Attribute 6: 0.8711\n",
      "  Attribute 7: 0.8103\n",
      "  Attribute 8: 0.8690\n",
      "  Attribute 9: 0.8487\n",
      "  Attribute 10: 0.9274\n",
      "  Attribute 11: 0.9391\n",
      "\n",
      "âœ… Epoch 3 Training Complete.\n",
      "  Average Loss: 2.9769\n",
      "  Overall Training Accuracy: 0.9084\n",
      "\n",
      "ðŸ§ª Validation phase:\n",
      "\n",
      "ðŸ“Š Validation Accuracy per Attribute:\n",
      "  Attribute 0 (val): 0.9367\n",
      "  Attribute 1 (val): 0.9651\n",
      "  Attribute 2 (val): 0.9512\n",
      "  Attribute 3 (val): 0.9825\n",
      "  Attribute 4 (val): 0.9796\n",
      "  Attribute 5 (val): 0.7813\n",
      "  Attribute 6 (val): 0.8257\n",
      "  Attribute 7 (val): 0.7992\n",
      "  Attribute 8 (val): 0.8549\n",
      "  Attribute 9 (val): 0.8325\n",
      "  Attribute 10 (val): 0.9155\n",
      "  Attribute 11 (val): 0.9311\n",
      "\n",
      "ðŸ“‰ Validation Loss: 3.4591\n",
      "âœ… Overall Validation Accuracy: 0.8963\n",
      "\n",
      "==================== Epoch 4/5 ====================\n",
      "\n",
      "ðŸ”§ Training phase:\n",
      "  Batch [10/2128], Loss: 2.0813\n",
      "  Batch [20/2128], Loss: 2.0621\n",
      "  Batch [30/2128], Loss: 2.2972\n",
      "  Batch [40/2128], Loss: 2.2222\n",
      "  Batch [50/2128], Loss: 2.4267\n",
      "  Batch [60/2128], Loss: 2.9649\n",
      "  Batch [70/2128], Loss: 3.4669\n",
      "  Batch [80/2128], Loss: 2.7365\n",
      "  Batch [90/2128], Loss: 2.7544\n",
      "  Batch [100/2128], Loss: 2.3440\n",
      "  Batch [110/2128], Loss: 2.3515\n",
      "  Batch [120/2128], Loss: 2.1443\n",
      "  Batch [130/2128], Loss: 2.3849\n",
      "  Batch [140/2128], Loss: 2.6806\n",
      "  Batch [150/2128], Loss: 2.1544\n",
      "  Batch [160/2128], Loss: 2.9423\n",
      "  Batch [170/2128], Loss: 2.7814\n",
      "  Batch [180/2128], Loss: 3.1169\n",
      "  Batch [190/2128], Loss: 2.4097\n",
      "  Batch [200/2128], Loss: 1.6142\n",
      "  Batch [210/2128], Loss: 2.1808\n",
      "  Batch [220/2128], Loss: 2.2789\n",
      "  Batch [230/2128], Loss: 2.8442\n",
      "  Batch [240/2128], Loss: 1.7370\n",
      "  Batch [250/2128], Loss: 2.5961\n",
      "  Batch [260/2128], Loss: 1.9647\n",
      "  Batch [270/2128], Loss: 1.6776\n",
      "  Batch [280/2128], Loss: 2.5566\n",
      "  Batch [290/2128], Loss: 3.5884\n",
      "  Batch [300/2128], Loss: 2.5961\n",
      "  Batch [310/2128], Loss: 2.3635\n",
      "  Batch [320/2128], Loss: 2.6612\n",
      "  Batch [330/2128], Loss: 2.1660\n",
      "  Batch [340/2128], Loss: 2.8273\n",
      "  Batch [350/2128], Loss: 3.3489\n",
      "  Batch [360/2128], Loss: 2.2552\n",
      "  Batch [370/2128], Loss: 3.0258\n",
      "  Batch [380/2128], Loss: 2.4919\n",
      "  Batch [390/2128], Loss: 1.8474\n",
      "  Batch [400/2128], Loss: 3.1174\n",
      "  Batch [410/2128], Loss: 2.2180\n",
      "  Batch [420/2128], Loss: 2.0734\n",
      "  Batch [430/2128], Loss: 2.7841\n",
      "  Batch [440/2128], Loss: 1.7569\n",
      "  Batch [450/2128], Loss: 1.9210\n",
      "  Batch [460/2128], Loss: 2.5781\n",
      "  Batch [470/2128], Loss: 1.7753\n",
      "  Batch [480/2128], Loss: 2.1610\n",
      "  Batch [490/2128], Loss: 2.5645\n",
      "  Batch [500/2128], Loss: 2.7929\n",
      "  Batch [510/2128], Loss: 2.2313\n",
      "  Batch [520/2128], Loss: 2.0051\n",
      "  Batch [530/2128], Loss: 2.0807\n",
      "  Batch [540/2128], Loss: 1.9245\n",
      "  Batch [550/2128], Loss: 2.5997\n",
      "  Batch [560/2128], Loss: 2.2756\n",
      "  Batch [570/2128], Loss: 1.9859\n",
      "  Batch [580/2128], Loss: 2.3717\n",
      "  Batch [590/2128], Loss: 3.2890\n",
      "  Batch [600/2128], Loss: 3.2703\n",
      "  Batch [610/2128], Loss: 2.0732\n",
      "  Batch [620/2128], Loss: 1.5659\n",
      "  Batch [630/2128], Loss: 1.8867\n",
      "  Batch [640/2128], Loss: 1.6783\n",
      "  Batch [650/2128], Loss: 2.5052\n",
      "  Batch [660/2128], Loss: 2.5644\n",
      "  Batch [670/2128], Loss: 2.6656\n",
      "  Batch [680/2128], Loss: 1.7487\n",
      "  Batch [690/2128], Loss: 2.2910\n",
      "  Batch [700/2128], Loss: 3.2926\n",
      "  Batch [710/2128], Loss: 1.9980\n",
      "  Batch [720/2128], Loss: 1.7102\n",
      "  Batch [730/2128], Loss: 2.6831\n",
      "  Batch [740/2128], Loss: 2.8387\n",
      "  Batch [750/2128], Loss: 1.8149\n",
      "  Batch [760/2128], Loss: 2.0318\n",
      "  Batch [770/2128], Loss: 1.8269\n",
      "  Batch [780/2128], Loss: 2.3280\n",
      "  Batch [790/2128], Loss: 2.4321\n",
      "  Batch [800/2128], Loss: 2.8810\n",
      "  Batch [810/2128], Loss: 2.4292\n",
      "  Batch [820/2128], Loss: 2.3238\n",
      "  Batch [830/2128], Loss: 2.0739\n",
      "  Batch [840/2128], Loss: 3.0539\n",
      "  Batch [850/2128], Loss: 2.5583\n",
      "  Batch [860/2128], Loss: 2.6552\n",
      "  Batch [870/2128], Loss: 3.1203\n",
      "  Batch [880/2128], Loss: 3.0872\n",
      "  Batch [890/2128], Loss: 1.2999\n",
      "  Batch [900/2128], Loss: 2.3377\n",
      "  Batch [910/2128], Loss: 1.8625\n",
      "  Batch [920/2128], Loss: 2.7141\n",
      "  Batch [930/2128], Loss: 2.1265\n",
      "  Batch [940/2128], Loss: 2.1793\n",
      "  Batch [950/2128], Loss: 2.4115\n",
      "  Batch [960/2128], Loss: 2.9939\n",
      "  Batch [970/2128], Loss: 2.5409\n",
      "  Batch [980/2128], Loss: 2.4870\n",
      "  Batch [990/2128], Loss: 2.8498\n",
      "  Batch [1000/2128], Loss: 2.1693\n",
      "  Batch [1010/2128], Loss: 3.1191\n",
      "  Batch [1020/2128], Loss: 1.6141\n",
      "  Batch [1030/2128], Loss: 2.6398\n",
      "  Batch [1040/2128], Loss: 3.0988\n",
      "  Batch [1050/2128], Loss: 2.4619\n",
      "  Batch [1060/2128], Loss: 2.7740\n",
      "  Batch [1070/2128], Loss: 2.6952\n",
      "  Batch [1080/2128], Loss: 3.7574\n",
      "  Batch [1090/2128], Loss: 2.6345\n",
      "  Batch [1100/2128], Loss: 2.5544\n",
      "  Batch [1110/2128], Loss: 2.6492\n",
      "  Batch [1120/2128], Loss: 2.5395\n",
      "  Batch [1130/2128], Loss: 2.8260\n",
      "  Batch [1140/2128], Loss: 2.8306\n",
      "  Batch [1150/2128], Loss: 2.5482\n",
      "  Batch [1160/2128], Loss: 2.7725\n",
      "  Batch [1170/2128], Loss: 2.1154\n",
      "  Batch [1180/2128], Loss: 2.4064\n",
      "  Batch [1190/2128], Loss: 2.1597\n",
      "  Batch [1200/2128], Loss: 2.9494\n",
      "  Batch [1210/2128], Loss: 2.8497\n",
      "  Batch [1220/2128], Loss: 3.1923\n",
      "  Batch [1230/2128], Loss: 2.6386\n",
      "  Batch [1240/2128], Loss: 2.7775\n",
      "  Batch [1250/2128], Loss: 2.5445\n",
      "  Batch [1260/2128], Loss: 1.8467\n",
      "  Batch [1270/2128], Loss: 2.0726\n",
      "  Batch [1280/2128], Loss: 3.6441\n",
      "  Batch [1290/2128], Loss: 2.1298\n",
      "  Batch [1300/2128], Loss: 3.4527\n",
      "  Batch [1310/2128], Loss: 2.8010\n",
      "  Batch [1320/2128], Loss: 3.6040\n",
      "  Batch [1330/2128], Loss: 3.3662\n",
      "  Batch [1340/2128], Loss: 3.8770\n",
      "  Batch [1350/2128], Loss: 2.5522\n",
      "  Batch [1360/2128], Loss: 2.2013\n",
      "  Batch [1370/2128], Loss: 1.9734\n",
      "  Batch [1380/2128], Loss: 1.5948\n",
      "  Batch [1390/2128], Loss: 2.7727\n",
      "  Batch [1400/2128], Loss: 4.1577\n",
      "  Batch [1410/2128], Loss: 2.8015\n",
      "  Batch [1420/2128], Loss: 3.0745\n",
      "  Batch [1430/2128], Loss: 1.9429\n",
      "  Batch [1440/2128], Loss: 1.6200\n",
      "  Batch [1450/2128], Loss: 2.6850\n",
      "  Batch [1460/2128], Loss: 1.8000\n",
      "  Batch [1470/2128], Loss: 1.7438\n",
      "  Batch [1480/2128], Loss: 3.3976\n",
      "  Batch [1490/2128], Loss: 2.7073\n",
      "  Batch [1500/2128], Loss: 2.0969\n",
      "  Batch [1510/2128], Loss: 2.2634\n",
      "  Batch [1520/2128], Loss: 1.8819\n",
      "  Batch [1530/2128], Loss: 2.4819\n",
      "  Batch [1540/2128], Loss: 2.5869\n",
      "  Batch [1550/2128], Loss: 1.9454\n",
      "  Batch [1560/2128], Loss: 2.0208\n",
      "  Batch [1570/2128], Loss: 1.7356\n",
      "  Batch [1580/2128], Loss: 1.8082\n",
      "  Batch [1590/2128], Loss: 2.6123\n",
      "  Batch [1600/2128], Loss: 2.4813\n",
      "  Batch [1610/2128], Loss: 2.4869\n",
      "  Batch [1620/2128], Loss: 4.1750\n",
      "  Batch [1630/2128], Loss: 3.4188\n",
      "  Batch [1640/2128], Loss: 2.0154\n",
      "  Batch [1650/2128], Loss: 2.1858\n",
      "  Batch [1660/2128], Loss: 2.4490\n",
      "  Batch [1670/2128], Loss: 2.7230\n",
      "  Batch [1680/2128], Loss: 3.0888\n",
      "  Batch [1690/2128], Loss: 2.2519\n",
      "  Batch [1700/2128], Loss: 3.0665\n",
      "  Batch [1710/2128], Loss: 3.1444\n",
      "  Batch [1720/2128], Loss: 2.6099\n",
      "  Batch [1730/2128], Loss: 2.4496\n",
      "  Batch [1740/2128], Loss: 3.3622\n",
      "  Batch [1750/2128], Loss: 3.1214\n",
      "  Batch [1760/2128], Loss: 2.1450\n",
      "  Batch [1770/2128], Loss: 3.2522\n",
      "  Batch [1780/2128], Loss: 2.1930\n",
      "  Batch [1790/2128], Loss: 1.7465\n",
      "  Batch [1800/2128], Loss: 2.8218\n",
      "  Batch [1810/2128], Loss: 1.5347\n",
      "  Batch [1820/2128], Loss: 2.2042\n",
      "  Batch [1830/2128], Loss: 2.0734\n",
      "  Batch [1840/2128], Loss: 2.6754\n",
      "  Batch [1850/2128], Loss: 3.7364\n",
      "  Batch [1860/2128], Loss: 2.0642\n",
      "  Batch [1870/2128], Loss: 2.6107\n",
      "  Batch [1880/2128], Loss: 2.2339\n",
      "  Batch [1890/2128], Loss: 1.6610\n",
      "  Batch [1900/2128], Loss: 2.1139\n",
      "  Batch [1910/2128], Loss: 2.9507\n",
      "  Batch [1920/2128], Loss: 1.9198\n",
      "  Batch [1930/2128], Loss: 4.2860\n",
      "  Batch [1940/2128], Loss: 2.0289\n",
      "  Batch [1950/2128], Loss: 2.7152\n",
      "  Batch [1960/2128], Loss: 3.0433\n",
      "  Batch [1970/2128], Loss: 2.0757\n",
      "  Batch [1980/2128], Loss: 2.3089\n",
      "  Batch [1990/2128], Loss: 2.7371\n",
      "  Batch [2000/2128], Loss: 2.6480\n",
      "  Batch [2010/2128], Loss: 3.2492\n",
      "  Batch [2020/2128], Loss: 2.9062\n",
      "  Batch [2030/2128], Loss: 2.0807\n",
      "  Batch [2040/2128], Loss: 1.8618\n",
      "  Batch [2050/2128], Loss: 3.4618\n",
      "  Batch [2060/2128], Loss: 2.5885\n",
      "  Batch [2070/2128], Loss: 1.9773\n",
      "  Batch [2080/2128], Loss: 3.0267\n",
      "  Batch [2090/2128], Loss: 3.4702\n",
      "  Batch [2100/2128], Loss: 3.9217\n",
      "  Batch [2110/2128], Loss: 2.7247\n",
      "  Batch [2120/2128], Loss: 2.9863\n",
      "  Batch [2128/2128], Loss: 16.0037\n",
      "\n",
      "ðŸ“Š Training Accuracy per Attribute:\n",
      "  Attribute 0: 0.9584\n",
      "  Attribute 1: 0.9724\n",
      "  Attribute 2: 0.9709\n",
      "  Attribute 3: 0.9837\n",
      "  Attribute 4: 0.9822\n",
      "  Attribute 5: 0.8271\n",
      "  Attribute 6: 0.8853\n",
      "  Attribute 7: 0.8319\n",
      "  Attribute 8: 0.8900\n",
      "  Attribute 9: 0.8700\n",
      "  Attribute 10: 0.9401\n",
      "  Attribute 11: 0.9475\n",
      "\n",
      "âœ… Epoch 4 Training Complete.\n",
      "  Average Loss: 2.5553\n",
      "  Overall Training Accuracy: 0.9216\n",
      "\n",
      "ðŸ§ª Validation phase:\n",
      "\n",
      "ðŸ“Š Validation Accuracy per Attribute:\n",
      "  Attribute 0 (val): 0.9392\n",
      "  Attribute 1 (val): 0.9613\n",
      "  Attribute 2 (val): 0.9605\n",
      "  Attribute 3 (val): 0.9839\n",
      "  Attribute 4 (val): 0.9805\n",
      "  Attribute 5 (val): 0.7865\n",
      "  Attribute 6 (val): 0.8527\n",
      "  Attribute 7 (val): 0.8109\n",
      "  Attribute 8 (val): 0.8542\n",
      "  Attribute 9 (val): 0.8236\n",
      "  Attribute 10 (val): 0.9169\n",
      "  Attribute 11 (val): 0.9258\n",
      "\n",
      "ðŸ“‰ Validation Loss: 3.4001\n",
      "âœ… Overall Validation Accuracy: 0.8997\n",
      "\n",
      "==================== Epoch 5/5 ====================\n",
      "\n",
      "ðŸ”§ Training phase:\n",
      "  Batch [10/2128], Loss: 3.5844\n",
      "  Batch [20/2128], Loss: 2.5015\n",
      "  Batch [30/2128], Loss: 2.0355\n",
      "  Batch [40/2128], Loss: 1.7970\n",
      "  Batch [50/2128], Loss: 1.8843\n",
      "  Batch [60/2128], Loss: 1.4028\n",
      "  Batch [70/2128], Loss: 2.1216\n",
      "  Batch [80/2128], Loss: 1.9376\n",
      "  Batch [90/2128], Loss: 3.0043\n",
      "  Batch [100/2128], Loss: 1.7246\n",
      "  Batch [110/2128], Loss: 1.1393\n",
      "  Batch [120/2128], Loss: 3.4962\n",
      "  Batch [130/2128], Loss: 2.1077\n",
      "  Batch [140/2128], Loss: 2.8316\n",
      "  Batch [150/2128], Loss: 2.1499\n",
      "  Batch [160/2128], Loss: 1.2135\n",
      "  Batch [170/2128], Loss: 1.9383\n",
      "  Batch [180/2128], Loss: 2.0085\n",
      "  Batch [190/2128], Loss: 1.3562\n",
      "  Batch [200/2128], Loss: 2.2336\n",
      "  Batch [210/2128], Loss: 1.7165\n",
      "  Batch [220/2128], Loss: 2.2515\n",
      "  Batch [230/2128], Loss: 1.5285\n",
      "  Batch [240/2128], Loss: 1.5866\n",
      "  Batch [250/2128], Loss: 1.7103\n",
      "  Batch [260/2128], Loss: 2.4393\n",
      "  Batch [270/2128], Loss: 1.9124\n",
      "  Batch [280/2128], Loss: 2.0452\n",
      "  Batch [290/2128], Loss: 1.2849\n",
      "  Batch [300/2128], Loss: 1.8770\n",
      "  Batch [310/2128], Loss: 1.6615\n",
      "  Batch [320/2128], Loss: 1.8666\n",
      "  Batch [330/2128], Loss: 1.8062\n",
      "  Batch [340/2128], Loss: 3.1729\n",
      "  Batch [350/2128], Loss: 2.2924\n",
      "  Batch [360/2128], Loss: 1.6441\n",
      "  Batch [370/2128], Loss: 1.5571\n",
      "  Batch [380/2128], Loss: 2.1682\n",
      "  Batch [390/2128], Loss: 1.8479\n",
      "  Batch [400/2128], Loss: 2.1195\n",
      "  Batch [410/2128], Loss: 2.0356\n",
      "  Batch [420/2128], Loss: 2.8973\n",
      "  Batch [430/2128], Loss: 2.1267\n",
      "  Batch [440/2128], Loss: 2.4748\n",
      "  Batch [450/2128], Loss: 2.8273\n",
      "  Batch [460/2128], Loss: 2.9622\n",
      "  Batch [470/2128], Loss: 3.0162\n",
      "  Batch [480/2128], Loss: 1.8193\n",
      "  Batch [490/2128], Loss: 1.9811\n",
      "  Batch [500/2128], Loss: 2.2505\n",
      "  Batch [510/2128], Loss: 1.7069\n",
      "  Batch [520/2128], Loss: 3.2968\n",
      "  Batch [530/2128], Loss: 1.8884\n",
      "  Batch [540/2128], Loss: 2.4078\n",
      "  Batch [550/2128], Loss: 2.0610\n",
      "  Batch [560/2128], Loss: 2.0250\n",
      "  Batch [570/2128], Loss: 1.5125\n",
      "  Batch [580/2128], Loss: 1.8901\n",
      "  Batch [590/2128], Loss: 2.3311\n",
      "  Batch [600/2128], Loss: 1.7031\n",
      "  Batch [610/2128], Loss: 1.7430\n",
      "  Batch [620/2128], Loss: 1.3247\n",
      "  Batch [630/2128], Loss: 1.4024\n",
      "  Batch [640/2128], Loss: 1.7546\n",
      "  Batch [650/2128], Loss: 2.9477\n",
      "  Batch [660/2128], Loss: 2.0460\n",
      "  Batch [670/2128], Loss: 2.8873\n",
      "  Batch [680/2128], Loss: 2.3266\n",
      "  Batch [690/2128], Loss: 1.9488\n",
      "  Batch [700/2128], Loss: 2.6172\n",
      "  Batch [710/2128], Loss: 1.4270\n",
      "  Batch [720/2128], Loss: 2.6763\n",
      "  Batch [730/2128], Loss: 1.6699\n",
      "  Batch [740/2128], Loss: 2.1812\n",
      "  Batch [750/2128], Loss: 2.4976\n",
      "  Batch [760/2128], Loss: 2.4683\n",
      "  Batch [770/2128], Loss: 1.3511\n",
      "  Batch [780/2128], Loss: 2.5017\n",
      "  Batch [790/2128], Loss: 1.7697\n",
      "  Batch [800/2128], Loss: 1.8101\n",
      "  Batch [810/2128], Loss: 2.3606\n",
      "  Batch [820/2128], Loss: 2.3866\n",
      "  Batch [830/2128], Loss: 2.8635\n",
      "  Batch [840/2128], Loss: 2.1629\n",
      "  Batch [850/2128], Loss: 1.6056\n",
      "  Batch [860/2128], Loss: 2.7171\n",
      "  Batch [870/2128], Loss: 2.3266\n",
      "  Batch [880/2128], Loss: 2.3432\n",
      "  Batch [890/2128], Loss: 1.7283\n",
      "  Batch [900/2128], Loss: 1.4986\n",
      "  Batch [910/2128], Loss: 2.3955\n",
      "  Batch [920/2128], Loss: 2.0610\n",
      "  Batch [930/2128], Loss: 2.0368\n",
      "  Batch [940/2128], Loss: 2.0397\n",
      "  Batch [950/2128], Loss: 1.8207\n",
      "  Batch [960/2128], Loss: 1.3467\n",
      "  Batch [970/2128], Loss: 2.3568\n",
      "  Batch [980/2128], Loss: 2.2670\n",
      "  Batch [990/2128], Loss: 2.1618\n",
      "  Batch [1000/2128], Loss: 1.8658\n",
      "  Batch [1010/2128], Loss: 3.2657\n",
      "  Batch [1020/2128], Loss: 2.2205\n",
      "  Batch [1030/2128], Loss: 2.5043\n",
      "  Batch [1040/2128], Loss: 1.4734\n",
      "  Batch [1050/2128], Loss: 1.8666\n",
      "  Batch [1060/2128], Loss: 2.0592\n",
      "  Batch [1070/2128], Loss: 1.3066\n",
      "  Batch [1080/2128], Loss: 1.6291\n",
      "  Batch [1090/2128], Loss: 2.5098\n",
      "  Batch [1100/2128], Loss: 2.1549\n",
      "  Batch [1110/2128], Loss: 2.2641\n",
      "  Batch [1120/2128], Loss: 1.4965\n",
      "  Batch [1130/2128], Loss: 1.2446\n",
      "  Batch [1140/2128], Loss: 2.6220\n",
      "  Batch [1150/2128], Loss: 2.5052\n",
      "  Batch [1160/2128], Loss: 2.4459\n",
      "  Batch [1170/2128], Loss: 1.9750\n",
      "  Batch [1180/2128], Loss: 2.1531\n",
      "  Batch [1190/2128], Loss: 2.3922\n",
      "  Batch [1200/2128], Loss: 1.8243\n",
      "  Batch [1210/2128], Loss: 1.4401\n",
      "  Batch [1220/2128], Loss: 1.6341\n",
      "  Batch [1230/2128], Loss: 2.1161\n",
      "  Batch [1240/2128], Loss: 2.0672\n",
      "  Batch [1250/2128], Loss: 1.9711\n",
      "  Batch [1260/2128], Loss: 2.4960\n",
      "  Batch [1270/2128], Loss: 1.9538\n",
      "  Batch [1280/2128], Loss: 1.8358\n",
      "  Batch [1290/2128], Loss: 1.7423\n",
      "  Batch [1300/2128], Loss: 2.4757\n",
      "  Batch [1310/2128], Loss: 1.4583\n",
      "  Batch [1320/2128], Loss: 1.9875\n",
      "  Batch [1330/2128], Loss: 1.5402\n",
      "  Batch [1340/2128], Loss: 2.7495\n",
      "  Batch [1350/2128], Loss: 2.1098\n",
      "  Batch [1360/2128], Loss: 2.7200\n",
      "  Batch [1370/2128], Loss: 2.0880\n",
      "  Batch [1380/2128], Loss: 2.2030\n",
      "  Batch [1390/2128], Loss: 1.9095\n",
      "  Batch [1400/2128], Loss: 2.9473\n",
      "  Batch [1410/2128], Loss: 2.6340\n",
      "  Batch [1420/2128], Loss: 2.1617\n",
      "  Batch [1430/2128], Loss: 1.6273\n",
      "  Batch [1440/2128], Loss: 1.9106\n",
      "  Batch [1450/2128], Loss: 1.9946\n",
      "  Batch [1460/2128], Loss: 2.8386\n",
      "  Batch [1470/2128], Loss: 3.0293\n",
      "  Batch [1480/2128], Loss: 1.2546\n",
      "  Batch [1490/2128], Loss: 1.6531\n",
      "  Batch [1500/2128], Loss: 2.0011\n",
      "  Batch [1510/2128], Loss: 1.6938\n",
      "  Batch [1520/2128], Loss: 1.6112\n",
      "  Batch [1530/2128], Loss: 1.3763\n",
      "  Batch [1540/2128], Loss: 2.5702\n",
      "  Batch [1550/2128], Loss: 1.9142\n",
      "  Batch [1560/2128], Loss: 2.3545\n",
      "  Batch [1570/2128], Loss: 2.6392\n",
      "  Batch [1580/2128], Loss: 1.8755\n",
      "  Batch [1590/2128], Loss: 3.0646\n",
      "  Batch [1600/2128], Loss: 2.9517\n",
      "  Batch [1610/2128], Loss: 2.3646\n",
      "  Batch [1620/2128], Loss: 2.2605\n",
      "  Batch [1630/2128], Loss: 2.3529\n",
      "  Batch [1640/2128], Loss: 1.7281\n",
      "  Batch [1650/2128], Loss: 2.9571\n",
      "  Batch [1660/2128], Loss: 1.9818\n",
      "  Batch [1670/2128], Loss: 2.4332\n",
      "  Batch [1680/2128], Loss: 1.8379\n",
      "  Batch [1690/2128], Loss: 2.2981\n",
      "  Batch [1700/2128], Loss: 2.3338\n",
      "  Batch [1710/2128], Loss: 2.1171\n",
      "  Batch [1720/2128], Loss: 2.6382\n",
      "  Batch [1730/2128], Loss: 2.0788\n",
      "  Batch [1740/2128], Loss: 2.4041\n",
      "  Batch [1750/2128], Loss: 1.5177\n",
      "  Batch [1760/2128], Loss: 1.9928\n",
      "  Batch [1770/2128], Loss: 2.1067\n",
      "  Batch [1780/2128], Loss: 2.2099\n",
      "  Batch [1790/2128], Loss: 2.7150\n",
      "  Batch [1800/2128], Loss: 2.4098\n",
      "  Batch [1810/2128], Loss: 2.4274\n",
      "  Batch [1820/2128], Loss: 1.4264\n",
      "  Batch [1830/2128], Loss: 2.0783\n",
      "  Batch [1840/2128], Loss: 1.9386\n",
      "  Batch [1850/2128], Loss: 2.8021\n",
      "  Batch [1860/2128], Loss: 3.5451\n",
      "  Batch [1870/2128], Loss: 3.1174\n",
      "  Batch [1880/2128], Loss: 1.5200\n",
      "  Batch [1890/2128], Loss: 1.9738\n",
      "  Batch [1900/2128], Loss: 2.5525\n",
      "  Batch [1910/2128], Loss: 2.3825\n",
      "  Batch [1920/2128], Loss: 1.5276\n",
      "  Batch [1930/2128], Loss: 2.7676\n",
      "  Batch [1940/2128], Loss: 2.0360\n",
      "  Batch [1950/2128], Loss: 2.3235\n",
      "  Batch [1960/2128], Loss: 2.4694\n",
      "  Batch [1970/2128], Loss: 1.4683\n",
      "  Batch [1980/2128], Loss: 1.5942\n",
      "  Batch [1990/2128], Loss: 3.0213\n",
      "  Batch [2000/2128], Loss: 2.2406\n",
      "  Batch [2010/2128], Loss: 2.0919\n",
      "  Batch [2020/2128], Loss: 2.9232\n",
      "  Batch [2030/2128], Loss: 2.3475\n",
      "  Batch [2040/2128], Loss: 1.4781\n",
      "  Batch [2050/2128], Loss: 2.5209\n",
      "  Batch [2060/2128], Loss: 1.6352\n",
      "  Batch [2070/2128], Loss: 1.4256\n",
      "  Batch [2080/2128], Loss: 1.9560\n",
      "  Batch [2090/2128], Loss: 2.0366\n",
      "  Batch [2100/2128], Loss: 1.3846\n",
      "  Batch [2110/2128], Loss: 1.6683\n",
      "  Batch [2120/2128], Loss: 3.0229\n",
      "  Batch [2128/2128], Loss: 3.3648\n",
      "\n",
      "ðŸ“Š Training Accuracy per Attribute:\n",
      "  Attribute 0: 0.9663\n",
      "  Attribute 1: 0.9773\n",
      "  Attribute 2: 0.9761\n",
      "  Attribute 3: 0.9854\n",
      "  Attribute 4: 0.9852\n",
      "  Attribute 5: 0.8558\n",
      "  Attribute 6: 0.9045\n",
      "  Attribute 7: 0.8587\n",
      "  Attribute 8: 0.9092\n",
      "  Attribute 9: 0.8964\n",
      "  Attribute 10: 0.9484\n",
      "  Attribute 11: 0.9568\n",
      "\n",
      "âœ… Epoch 5 Training Complete.\n",
      "  Average Loss: 2.1173\n",
      "  Overall Training Accuracy: 0.9350\n",
      "\n",
      "ðŸ§ª Validation phase:\n",
      "\n",
      "ðŸ“Š Validation Accuracy per Attribute:\n",
      "  Attribute 0 (val): 0.9332\n",
      "  Attribute 1 (val): 0.9605\n",
      "  Attribute 2 (val): 0.9613\n",
      "  Attribute 3 (val): 0.9825\n",
      "  Attribute 4 (val): 0.9805\n",
      "  Attribute 5 (val): 0.7925\n",
      "  Attribute 6 (val): 0.8614\n",
      "  Attribute 7 (val): 0.8088\n",
      "  Attribute 8 (val): 0.8629\n",
      "  Attribute 9 (val): 0.8197\n",
      "  Attribute 10 (val): 0.9191\n",
      "  Attribute 11 (val): 0.9271\n",
      "\n",
      "ðŸ“‰ Validation Loss: 3.4532\n",
      "âœ… Overall Validation Accuracy: 0.9008\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 5  # Change as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n==================== Epoch {epoch+1}/{num_epochs} ====================\")\n",
    "\n",
    "    # ---------- Training ----------\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    print(\"\\nðŸ”§ Training phase:\")\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)  # list of 12 outputs\n",
    "        loss = sum(criterion(o, labels[:, i].long()) for i, o in enumerate(outputs))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Store labels and predictions\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        preds = torch.stack([torch.argmax(o, dim=1) for o in outputs], dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "\n",
    "        # Debugging info for first batch of first epoch\n",
    "        if epoch == 0 and batch_idx == 0:\n",
    "            print(\"  â†’ Output (first attribute):\", outputs[0][0])\n",
    "            print(\"  â†’ Label (first sample):\", labels[0])\n",
    "            print(\"  â†’ Loss:\", loss.item())\n",
    "\n",
    "        if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            print(f\"  Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "\n",
    "    print(\"\\nðŸ“Š Training Accuracy per Attribute:\")\n",
    "    accuracy_per_attribute = []\n",
    "    for i in range(all_labels.shape[1]):\n",
    "        acc = accuracy_score(all_labels[:, i], all_preds[:, i])\n",
    "        accuracy_per_attribute.append(acc)\n",
    "        print(f\"  Attribute {i}: {acc:.4f}\")\n",
    "\n",
    "    overall_accuracy = np.mean(accuracy_per_attribute)\n",
    "    print(f\"\\nâœ… Epoch {epoch+1} Training Complete.\")\n",
    "    print(f\"  Average Loss: {running_loss / len(train_loader):.4f}\")\n",
    "    print(f\"  Overall Training Accuracy: {overall_accuracy:.4f}\")\n",
    "\n",
    "    # ---------- Validation ----------\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "\n",
    "    print(\"\\nðŸ§ª Validation phase:\")\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels) in enumerate(val_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = sum(criterion(o, labels[:, i].long()) for i, o in enumerate(outputs))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            val_labels.append(labels.cpu().numpy())\n",
    "            preds = torch.stack([torch.argmax(o, dim=1) for o in outputs], dim=1)\n",
    "            val_preds.append(preds.cpu().numpy())\n",
    "\n",
    "    val_labels = np.concatenate(val_labels, axis=0)\n",
    "    val_preds = np.concatenate(val_preds, axis=0)\n",
    "\n",
    "    print(\"\\nðŸ“Š Validation Accuracy per Attribute:\")\n",
    "    accuracy_per_attribute = []\n",
    "    for i in range(val_labels.shape[1]):\n",
    "        acc = accuracy_score(val_labels[:, i], val_preds[:, i])\n",
    "        accuracy_per_attribute.append(acc)\n",
    "        print(f\"  Attribute {i} (val): {acc:.4f}\")\n",
    "\n",
    "    overall_val_accuracy = np.mean(accuracy_per_attribute)\n",
    "    print(f\"\\nðŸ“‰ Validation Loss: {val_loss / len(val_loader):.4f}\")\n",
    "    print(f\"âœ… Overall Validation Accuracy: {overall_val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eda14f-ac62-4a56-a063-89afc8eeaea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"classification/best_model.pth\")\n",
    "print(\"ðŸ“¦ Final model saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
